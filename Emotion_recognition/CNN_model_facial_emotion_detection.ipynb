{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/docs/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import gzip\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as ss\n",
    "import matplotlib.image as img\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"loading emotion labels from file\"\"\"\n",
    "lables_y = np.load('labels.npy')\n",
    "#print(lables_y)\n",
    "\"\"\"loading images - pixel matrices from file\"\"\"\n",
    "images = np.load('image_matrix.npy')\n",
    "images, lables_y = shuffle(images, lables_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"splitting to train and test\"\"\"\n",
    "train_images = images[0:300]\n",
    "#print(train_images.shape)\n",
    "test_images = images[300:327]\n",
    "#train_y = train_labels_arr.reshape(300,1)\n",
    "#test_y = test_labels_arr.reshape(27,1)\n",
    "train_y = lables_y[0:300]\n",
    "test_y = lables_y[300: 327]\n",
    "#print(train_y)\n",
    "\"\"\"normalization\"\"\"\n",
    "train_x_norm = (train_images / 255)\n",
    "test_x_norm = (test_images / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "\n",
    "def central_scale_images(X_imgs, y_images, scales):\n",
    "    # Various settings needed for Tensorflow operation\n",
    "    boxes = np.zeros((len(scales), 4), dtype=np.float32)\n",
    "    for index, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - 0.5 * scale  # To scale centrally\n",
    "        x2 = y2 = 0.5 + 0.5 * scale\n",
    "        boxes[index] = np.array([y1, x1, y2, x2], dtype=np.float32)\n",
    "    box_ind = np.zeros((len(scales)), dtype=np.int32)\n",
    "    crop_size = np.array([IMAGE_SIZE, IMAGE_SIZE], dtype=np.int32)\n",
    "    X_scale_data = []\n",
    "    y_scale_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape=(1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    # Define Tensorflow operation for all scales but only one base image at a time\n",
    "    tf_img = tf.image.crop_and_resize(X, boxes, box_ind, crop_size)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        counter = 0\n",
    "        for img_data in X_imgs:\n",
    "            # a,b,c = np.where(a == img_data)\n",
    "            batch_img = np.expand_dims(img_data, axis=0)\n",
    "            scaled_imgs = sess.run(tf_img, feed_dict={X: batch_img})\n",
    "\n",
    "            X_scale_data.extend(scaled_imgs)\n",
    "            for i in range(3):\n",
    "                y_scale_data.append(y_images[counter])\n",
    "            counter = counter + 1\n",
    "    X_scale_data = np.array(X_scale_data, dtype=np.float32)\n",
    "    y_scale_data = np.array(y_scale_data, dtype=np.float32)\n",
    "\n",
    "    return X_scale_data, y_scale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_images(X_imgs, y_images):\n",
    "    X_flip = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    tf_img1 = tf.image.flip_left_right(X)\n",
    "    tf_img2 = tf.image.flip_up_down(X)\n",
    "    tf_img3 = tf.image.transpose_image(X)\n",
    "    counter = 0\n",
    "    y_flip= []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            flipped_imgs = sess.run([tf_img1, tf_img2, tf_img3], feed_dict = {X: img})\n",
    "            X_flip.extend(flipped_imgs)\n",
    "            for i in range(3):\n",
    "                y_flip.append(y_images[counter])\n",
    "            counter = counter + 1\n",
    "    X_flip = np.array(X_flip, dtype = np.float32)\n",
    "    y_flip = np.array(y_flip, dtype=np.float32)\n",
    "    return X_flip, y_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x_norm1 = train_x_norm.reshape(train_x_norm.shape[0], train_x_norm.shape[1], train_x_norm.shape[2], 1)\n",
    "test_x_norm1 = test_x_norm.reshape(test_x_norm.shape[0], test_x_norm.shape[1], test_x_norm.shape[2], 1)\n",
    "\n",
    "# Produce each image at scaling of 90%, 75% and 60% of original image.\n",
    "\n",
    "X_imgs_train, y_imgs_train = central_scale_images(train_x_norm1 , lables_y, [0.90, 0.75, 0.60])\n",
    "X_flipped_train,y_flipped_train = flip_images(train_x_norm1 , lables_y)\n",
    "\n",
    "train_X = np.concatenate((X_imgs_train , X_flipped_train), axis=0)\n",
    "train_Y = np.concatenate((y_imgs_train, y_flipped_train), axis=0)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"reshaping X to contain channels = 1\"\"\"\n",
    "#print(train_x_norm.shape)\n",
    "train_x_norm1 = train_x_norm.reshape(train_x_norm.shape[0], train_x_norm.shape[1], train_x_norm.shape[2], 1)\n",
    "test_x_norm1 = test_x_norm.reshape(test_x_norm.shape[0], test_x_norm.shape[1], test_x_norm.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"one hot encoding function\"\"\"\n",
    "def One_Hot_Encoding(arr, samples_num):\n",
    "    encode_matrix = np.zeros((samples_num, 8))\n",
    "    for i in range(samples_num):\n",
    "        #print(type(arr[i][0]))\n",
    "        encode_matrix[i][int(float(arr[i][0]))] = 1\n",
    "    return encode_matrix\n",
    "\n",
    "\n",
    "train_y_encode = One_Hot_Encoding(train_Y, len(train_Y))\n",
    "test_y_encode = One_Hot_Encoding(test_y, len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"function to initialize weights and bias for convolution 2 layers\"\"\"\n",
    "def initialize_weights():\n",
    "    \n",
    "    tf.set_random_seed(1)                              \n",
    "        \n",
    "    W1 = tf.get_variable('W1',[5, 5, 1, 6], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable('W2',[5, 5, 6, 8], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W3 = tf.get_variable('W3',[5, 5, 8, 10], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "   # B1 = tf.get_variable('B1', tf.zeros([8], tf.float32), name=)\n",
    "    B1 = tf.get_variable('B1', [6], initializer=tf.zeros_initializer())\n",
    "    B2 = tf.get_variable('B2', [8], initializer=tf.zeros_initializer())\n",
    "    B3 = tf.get_variable('B3', [10], initializer=tf.zeros_initializer())\n",
    "   # B2 = tf.zeros([12], tf.float32)\n",
    "    #B3 = tf.zeros([16], tf.float32)\n",
    "    weights = ( W1, W2, W3)\n",
    "    bias = ( B1,B2, B3)\n",
    "    #print(\"weights intialized\")\n",
    "    #print(W1)\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"forward propagation. convolution and fully connected. dropout is applied as regularization for fully conencted layers\"\"\"\n",
    "\n",
    "\n",
    "def frwd_propagation(X_input, weights, bias):\n",
    "      \n",
    "    \n",
    "    W1, W2, W3 = weights\n",
    "    B1, B2, B3 = bias\n",
    "    # first convolution with relu as activation\n",
    "    conv1 = tf.nn.conv2d(X_input, W1, strides = [1,1,1,1], padding = 'SAME', name='conv1')\n",
    "    print(\"conv1 layer shape\")\n",
    "    print(conv1.shape)\n",
    "    Z1 = tf.nn.bias_add(conv1, B1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    #max pooling\n",
    "    pool1 = tf.nn.max_pool(A1, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    print(\"pool1 shape\")\n",
    "    print(pool1.shape)\n",
    "    # 2nd convolution layer with relu as activation\n",
    "    conv2 = tf.nn.conv2d(pool1, W2, strides = [1,1,1,1], padding = 'VALID', name='conv2')\n",
    "    Z2 = tf.nn.bias_add(conv2, B2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    print(\"conv2 shape\")\n",
    "    print(conv2.shape)\n",
    "    #avg pooling\n",
    "    pool2 = tf.nn.max_pool(A2, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    #pool2 = tf.nn.avg_pool(A2, ksize = [1,8,8,1], strides = [1,1,1,1], padding = 'VALID', data_format='NHWC', name=None)\n",
    "    print(\"pool2 shape before unfaltten\")\n",
    "    print(pool2.shape)\n",
    "    conv3 = tf.nn.conv2d(pool2, W3, strides = [1,1,1,1], padding = 'SAME', name='conv3')\n",
    "    Z3 = tf.nn.bias_add(conv3, B3)\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    print(\"conv3 shape\")\n",
    "    print(conv3.shape)\n",
    "    #pool3 = tf.nn.avg_pool(A3, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID', data_format='NHWC', name=None)\n",
    "    pool3 = tf.nn.max_pool(A3, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    print(\"pool3 shape before unfaltten\")\n",
    "    print(pool3.shape)\n",
    "    pool3 = tf.contrib.layers.flatten(pool3)\n",
    "    print(\"pool3 shape after flatten\")\n",
    "    print(pool3.shape)\n",
    "    #fully connected layer relu as activation\n",
    "    Z4 = tf.contrib.layers.fully_connected(pool3, 3000, activation_fn=None)\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    Z5 = tf.contrib.layers.fully_connected(A4, 400, activation_fn=None)\n",
    "    #A5 = tf.nn.relu(Z5)\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    #dropout regularization\n",
    "    #drop_out_A3 = tf.nn.dropout(A3, 0.95)\n",
    "    Z6 = tf.contrib.layers.fully_connected(A5, 8, activation_fn=None)\n",
    "    A6 = tf.nn.sigmoid(Z6)\n",
    "    \n",
    "    \n",
    "\n",
    "    return A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, iterations = 10, minibatch_size = 15):\n",
    "\n",
    "    tf.set_random_seed(1)                                                                       \n",
    "    (m, n_h, n_w, n_c) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = [] \n",
    "    seed = 3\n",
    "    weights, bias = initialize_weights()\n",
    "    W1, W2, W3 = weights\n",
    "    B1, B2, B3 = bias\n",
    "    #print(\"weights intialized\")\n",
    "    #print(W1)\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_h, n_w, n_c])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, n_y])\n",
    "    #forward propagation\n",
    "    A5 = frwd_propagation(X, weights, bias)\n",
    "    #calcualting cost\n",
    "    cost_total = tf.nn.softmax_cross_entropy_with_logits(logits = A5, labels = Y)\n",
    "    #l2 regularization\n",
    "    #regularizer = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2)\n",
    "    #cost = tf.reduce_mean(cost_total + 0.001 * regularizer)\n",
    "    cost = tf.reduce_mean(cost_total )\n",
    "    #back propagation\n",
    "    adamoptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    #soft = tf.nn.softmax(A5)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(tf.trainable_variables())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        #saver = tf.train.Saver([W1, W2, W3, B1, B2, B3])\n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            print(\"iteration is %s\" %iteration)\n",
    "            #Weights1 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'conv1/kernel')[0]\n",
    "            #Bias1 = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/bias')[0]\n",
    "            #print(\"W1\")\n",
    "            #print(Weights1)\n",
    "            cost_min_batch = 0\n",
    "            seed = seed + 1\n",
    "            \n",
    "            minbatches = generate_min_batch(X_train, Y_train, seed, minibatch_size)\n",
    "            minbatch_num = int(m / minibatch_size)\n",
    "            for batch in minbatches:\n",
    "\n",
    "                (min_batch_X, min_batch_Y) = batch\n",
    "                _ , cost_single_batch = sess.run([adamoptimizer, cost], feed_dict={X: min_batch_X, Y: min_batch_Y})\n",
    "                cost_min_batch += cost_single_batch / minbatch_num\n",
    "            #_, c = sess.run([adamoptimizer, cost], feed_dict={X: X_train, Y: Y_train})\n",
    "            print(\"iteration %s done\" %iteration)  \n",
    "            #gr = tf.get_default_graph()\n",
    "            #conv1_kernel_val = gr.get_tensor_by_name('conv1/kernel:0').eval()\n",
    "            #print(\"W1\")\n",
    "            #print(conv1_kernel_val)\n",
    "            if iteration % 1 == 0:\n",
    "                print(\"cost is %s \" %cost_min_batch)\n",
    "            \n",
    "            #if iteration % 1 == 0:\n",
    "                #costs.append(cost_min_batch) \n",
    "        print(\"iterations are done\")\n",
    "        \n",
    "\n",
    "        # Calculate correct predictions\n",
    "        predict_y = tf.argmax(A5, 1)\n",
    "        #pred = tf.argmax(soft, 1)\n",
    "        print(\"prediction y done\")\n",
    "        correct_prediction = tf.equal(predict_y, tf.argmax(Y, 1))\n",
    "        print(\"corect pred computed\")\n",
    "        \n",
    "        # Calculate accuracy on the TRAIN SET AND test set\n",
    "        accuracy = 100 * tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        #print(\"pred : %s\" %predict_y.eval({X: X_test, Y: Y_test}))\n",
    "        #print(\"Train Accuracy:\", train_accuracy)\n",
    "        #print(\"Test Accuracy:\", test_accuracy)\n",
    "        #new_y = predict_y.eval({X: X_train, Y: Y_train})\n",
    "        #print(new_y)\n",
    "        #saver.save(sess, 'my_test_model')\n",
    "        #save_path = saver.save(sess, 'C:/Users/sahitya/258_assignment2/258_project/my_test_model')\n",
    "        save_path = saver.save(sess, './my_test_model2')\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        #print(Weights1)       \n",
    "        return train_accuracy, test_accuracy, weights, bias, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_min_batch(X_train, Y_train, seed, minbatch_size):\n",
    "    m = X_train.shape[0]\n",
    "    #print(\"this is m %s\" %m)\n",
    "    np.random.seed(seed)\n",
    "    random_p = np.random.permutation(m)\n",
    "    #print(\"permutaion %s \" %random_p)\n",
    "    X_shuffled = X_train[random_p, :]\n",
    "    Y_shuffled = Y_train[random_p, :]\n",
    "    \n",
    "    num_min_batches = m/minbatch_size\n",
    "    min_batches = []\n",
    "    for i in range(0, int(num_min_batches)):\n",
    "        batch_X = X_shuffled[i * minbatch_size:(i + 1) * minbatch_size, :]\n",
    "        batch_Y = Y_shuffled[i * minbatch_size:(i + 1) * minbatch_size, : ] \n",
    "        min_full_batch = (batch_X, batch_Y)\n",
    "        min_batches.append(min_full_batch)\n",
    "    \n",
    "    return min_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 layer shape\n",
      "(?, 256, 256, 6)\n",
      "pool1 shape\n",
      "(?, 127, 127, 6)\n",
      "conv2 shape\n",
      "(?, 123, 123, 8)\n",
      "pool2 shape before unfaltten\n",
      "(?, 61, 61, 8)\n",
      "conv3 shape\n",
      "(?, 61, 61, 10)\n",
      "pool3 shape before unfaltten\n",
      "(?, 30, 30, 10)\n",
      "pool3 shape after flatten\n",
      "(?, 9000)\n",
      "WARNING:tensorflow:From <ipython-input-11-6e5656ce3c6c>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "iteration is 0\n",
      "iteration 0 done\n",
      "cost is 1.9195772240559261 \n",
      "iteration is 1\n",
      "iteration 1 done\n",
      "cost is 1.8367922912041352 \n",
      "iteration is 2\n",
      "iteration 2 done\n",
      "cost is 1.7217806021372486 \n",
      "iteration is 3\n",
      "iteration 3 done\n",
      "cost is 1.6049148162206015 \n",
      "iteration is 4\n",
      "iteration 4 done\n",
      "cost is 1.54725806415081 \n",
      "iteration is 5\n",
      "iteration 5 done\n",
      "cost is 1.5053022146224972 \n",
      "iteration is 6\n",
      "iteration 6 done\n",
      "cost is 1.4673484623432154 \n",
      "iteration is 7\n",
      "iteration 7 done\n",
      "cost is 1.4395515809456505 \n",
      "iteration is 8\n",
      "iteration 8 done\n",
      "cost is 1.4153697441021604 \n",
      "iteration is 9\n",
      "iteration 9 done\n",
      "cost is 1.393168182174364 \n",
      "iterations are done\n",
      "prediction y done\n",
      "corect pred computed\n",
      "Model saved in path: ./my_test_model2\n",
      "train accuracy : 89.55556\n",
      "test accuracy : 81.48148\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "train_accuracy, test_accuracy, weights, bias, costs = model(train_X, train_y_encode, test_x_norm1, test_y_encode)\n",
    "print(\"train accuracy : %s\" %train_accuracy)\n",
    "print(\"test accuracy : %s\" %test_accuracy)\n",
    "\n",
    "end = datetime.now() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
