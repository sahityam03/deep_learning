{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020,)\n",
      "(1020, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Reading data\"\"\"\n",
    "\n",
    "data_x = np.load('ex5_train_x.npy', mmap_mode='r')\n",
    "data_y = np.load('ex5_train_y.npy', mmap_mode='r')\n",
    "\n",
    "\"\"\"normalising data_x\"\"\"\n",
    "data_x1 = (data_x / 255) - 0.5\n",
    "print(data_y.shape)\n",
    "data_y1 = data_y.reshape(1020,1)\n",
    "print(data_y1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first filter set'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"first filter set\"\"\"\n",
    "\n",
    "#filter_set1 = 0.01 * np.random.randn(8, 4, 4)\n",
    "#filter_set2 = 0.01 * np.random.randn(16, 4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(data, padding):\n",
    "    m = len(data)\n",
    "    data_padded = np.zeros((data.shape[0], data.shape[1]+2, data.shape[2]+2, data.shape[3]))\n",
    "    for i in range(m):\n",
    "        data_T = data[i].T\n",
    "        data_temp = np.zeros((data_T.shape[0], data_T.shape[1]+2, data_T.shape[2]+2))\n",
    "        for j in range(0, len(data_T)):\n",
    "            temp = np.pad(data_T[j], (padding, padding), 'constant', constant_values=(0))\n",
    "            data_temp[j] = temp\n",
    "        data_padded[i] = data_temp.T\n",
    "    return data_padded\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(padding, stride, data, filter_size, filters, bias):\n",
    "    \n",
    "    m = len(data)\n",
    "    image_size = data.shape[1]\n",
    "    filters_number = filters.shape[3]\n",
    "    if(padding > 0):\n",
    "        data_aftr_pad = zero_padding(data_x1, padding)\n",
    "    else:\n",
    "        data_aftr_pad = data\n",
    "        \n",
    "    output_size = ((image_size + 2*padding - filter_size) / stride) + 1\n",
    "    #output_size = ((image_size - filter_size) / stride) + 1\n",
    "    #print(\"this is output size %s \" %output_size)\n",
    "    output = np.zeros((m, int(output_size), int(output_size), filters_number)) \n",
    "    n_h = output.shape[1]\n",
    "    n_w = output.shape[2]\n",
    "    n_ch = output.shape[3]\n",
    "    for i in range(m):\n",
    "        #data_each = data[i, :]\n",
    "        data_each = data_aftr_pad[i, :]\n",
    "        for h in range(n_h):\n",
    "            vert_start = h * stride\n",
    "            vert_end = h * stride + filter_size\n",
    "            if(vert_end > data_each.shape[0]):\n",
    "                break\n",
    "            for w in range(n_w):\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = w * stride + filter_size\n",
    "                if(horiz_end > data_each.shape[1]):\n",
    "                    break\n",
    "                for c in range(n_ch):\n",
    "                    #finding corner of the slice with filter size\n",
    "                    #vert_start = h * stride\n",
    "                    #vert_end = h * stride + filter_size\n",
    "                    #horiz_start = w * stride\n",
    "                    #horiz_end = w * stride + filter_size\n",
    "                   # print(\"vert start   %s\" %(vert_start))\n",
    "                    #print(\"vert  end %s\" %(vert_end))\n",
    "                    #print(\"hori start  %s\" %(horiz_start))\n",
    "                    #print(\"hori  end %s\" %(horiz_end))\n",
    "                    a_slice = data_each[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    #print(a_slice)\n",
    "                    output[i, h, w, c] = conv_single_step1(a_slice, filters[:, :, :, c], bias[:, :, :, c])\n",
    "    #print(\"endtime %s\")\n",
    "    #print(time.time())\n",
    "    cache = (data, filters, bias, padding, stride)\n",
    "    return output, cache      \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step1(a_slice_prev, W, bias):\n",
    "   \n",
    "    sum = np.multiply(a_slice_prev, W) + bias\n",
    "    Z = np.sum(sum)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"convolution single step - a filter(channel) and a part of data which is filtersize(data_part) is passed\"\"\"\n",
    "\n",
    "def conv_single_step(data_part, channel, bias):\n",
    "    #temp_matrix = np.multiply(data_part, channel)\n",
    "    m = channel.shape[0]\n",
    "    ch = data_part.shape[2]   #no.of channels in previous layer or in the slice\n",
    "    #print(\"this is no.of channels %s\" %ch)\n",
    "    temp = 0\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            for c in range(ch):\n",
    "                #print(data_part[i][j][c])\n",
    "                temp += data_part[i][j][c] * channel[i][j] \n",
    "    temp = temp + bias\n",
    "    return temp\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv1_data, cache_conv1 = conv_forward(1, 2, data_x1, 4, filter_set1, conv_bias1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(conv1_data.shape)\n",
    "#print(conv1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv1_data_relu = relu_activation(conv1_data)\n",
    "#print(conv1_data_relu.shape)\n",
    "#print(conv1_data_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pooling - mode is either 'max' or 'avg' \"\"\"\n",
    "\n",
    "def pool_forward(padding, stride, data, filter_size, mode):\n",
    "    m = len(data)\n",
    "    image_size = data.shape[1]\n",
    "    filters_number = data.shape[3]\n",
    "    output_size = ((image_size + 2 * padding - filter_size) / stride) + 1\n",
    "    output = np.zeros((m, int(output_size), int(output_size), filters_number)) \n",
    "    n_h = output.shape[1]\n",
    "    n_w = output.shape[2]\n",
    "    n_ch = output.shape[3]\n",
    "    for i in range(m):\n",
    "        for h in range(n_h):\n",
    "            vert_start = h * stride\n",
    "            vert_end = h * stride + filter_size\n",
    "            if(vert_end > data.shape[1]):\n",
    "                break\n",
    "            for w in range(n_w):\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = w * stride + filter_size\n",
    "                if(horiz_end > data.shape[2]):\n",
    "                    break\n",
    "                for c in range(n_ch):\n",
    "                    a_slice = data[i,vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    if (mode == 'max'):\n",
    "                        output[i, h, w, c] = np.max(a_slice)\n",
    "                    elif (mode == 'avg'):\n",
    "                        output[i, h, w, c] = np.mean(a_slice)\n",
    "    cache = (data, stride, padding, filter_size)\n",
    "    return output, cache                    \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool1_data, cache_pool1 = pool_forward(0, 1, conv1_data_relu, 5, 'max')\n",
    "#print(pool1_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##conv2_data, cache_conv2 = conv_forward(0, 2, pool1_data, 4, filter_set2, conv_bias2)\n",
    "#print(conv2_data.shape)\n",
    "#conv2_data_relu = relu_activation(conv2_data)\n",
    "#print(conv2_data_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool2_data, cache_pool2 = pool_forward(0, 1, conv2_data_relu, 5, 'max')\n",
    "#print(pool2_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples = pool2_data.shape[0]\n",
    "#flatten_size = pool2_data.shape[1] * pool2_data.shape[2] * pool2_data.shape[3]\n",
    "#flatten_data = pool2_data.reshape(samples, flatten_size )\n",
    "#print(flatten_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ReLU activation\"\"\"\n",
    "\n",
    "def relu_activation(data_array):\n",
    "    return np.maximum(data_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dRelu(data_array):\n",
    "    data_array[data_array<=0] = 0\n",
    "    data_array[data_array>0] = 1\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for One hot encoding. input Y as an array and no.of samples is passed to the function\"\"\"\n",
    "\n",
    "def One_Hot_Encoding(arr, samples_num):\n",
    "    encode_matrix = np.zeros((samples_num, 6))\n",
    "    for i in range(samples_num):\n",
    "        encode_matrix[i][arr[i][0]] = 1\n",
    "    return encode_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_en = One_Hot_Encoding(data_y1, len(data_y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_parameters():\n",
    "\n",
    "    #np.random.seed(1)\n",
    "    \n",
    "    W_first = 0.01 * np.random.randn(108, 1296)  # 108 X 1296\n",
    "    W_second = 0.01 * np.random.randn(6, 108)  # 6 X 108\n",
    "\n",
    "    \n",
    "    bias1 = np.zeros((108, 1)) # 108 X 1\n",
    "    bias2 = np.zeros((6, 1)) # 10 X 1\n",
    "    \n",
    "    \n",
    "    return W_first, W_second, bias1, bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardpropagation(W1, W2, A, b1, b2):\n",
    "    \n",
    "    \"\"\" performing forward propagation. returns new neurons and Zs\"\"\"\n",
    "\n",
    "    part1 = np.dot(A, W1.T)      #3500 x 25\n",
    "    Z1 = np.add(part1.T,b1)     #25 X 3500\n",
    "    #A1 = sigmoid(Z1)            #25 X 3500\n",
    "    A1 = relu_activation(Z1)\n",
    "    \n",
    "    part2 = np.dot(W2,A1) \n",
    "    Z2 = np.add(part2,b2)       #10 X 3500\n",
    "    A2 = sigmoid(Z2)           # 25 X 3500\n",
    "    A2 = A2.T                   #3500 X 10\n",
    "\n",
    "     \n",
    "    return A1, A2, Z1, Z2\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\" sigmoid function when an array is passed as parameter\"\"\"\n",
    "    \n",
    "    return 1/(1 + np.exp(-z))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dSigmoid(A):\n",
    "    \n",
    "    \"derivative of sigmoid function or sigmoid prime\"\n",
    "\n",
    "    return np.multiply(A, (1 - A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backwardpropagation(A1, A2, Z1, Y, X):\n",
    "    \n",
    "    \"\"\" backward propagation function. the function ouputs gradient of weights and bias\"\"\"\n",
    "    \n",
    "    m = len(A1)\n",
    "    dZ2 = A2 - Y   # 3500 X 10\n",
    "    #print(dZ2.shape)\n",
    "    dW2 = (1/m) * np.dot(dZ2.T, A1.T) # 10 X 25\n",
    "    db2 = (1/m) * np.sum(dZ2.T, axis=1, keepdims = True) \n",
    "    #gdashZ1 = dSigmoid(A1)  # 25 X 3500\n",
    "    gdashZ1 = dRelu(A1)\n",
    "    dZ1_firstpart = np.dot(dZ2, W2) # 3500 X 25\n",
    "    dZ1 = np.multiply(dZ1_firstpart.T, gdashZ1) # 25 X 3500\n",
    "    dW1 = (1/m) * np.dot(dZ1, X) # 25 X 400\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims = True) \n",
    "    dA0 = np.dot(dZ1.T, W1)\n",
    "        \n",
    "    return dW1, dW2, db1, db2, dA0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(dW1, dW2, db1, db2, B1, B2, W1, W2, alpha):\n",
    "    \n",
    "    \n",
    "    \"\"\" gradient descent calcualtes new weights and bais\"\"\"\n",
    "\n",
    "    \n",
    "    w2 = W2 - alpha * dW2\n",
    "    w1 = W1 - alpha * dW1\n",
    "    b1 = B1 - alpha * db1\n",
    "    b2 = B2 - alpha * db2\n",
    "    \n",
    "    return w1, w2, b1, b2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunction(y_en, final_A1):\n",
    "    \n",
    "    \"\"\" calculating loss functions - y (is given data Y) and y1(is A2) are arrays \"\"\"\n",
    "    import math\n",
    "    m = len(final_A1)\n",
    "    final_A = np.clip(final_A1, 0.0001, 0.99999)\n",
    "    loss = (-1/m) * np.sum(np.multiply(y_en, np.log(final_A)) + np.multiply(1 - y_en, np.log(1 - final_A)))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    \"\"\" softmax function final A2 is passed as parameter (e power x) / sum(e power x)\"\"\"\n",
    "    E = np.exp(x)\n",
    "    deno =  np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    deno = deno.reshape(deno.shape[0],1)\n",
    "    probs = E / deno\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W1, W2, B1, B2  = initalize_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A1, A2, Z1, Z2 = forwardpropagation(W1, W2, flatten_data, B1, B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(A1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = lossFunction(Y_en, A2)\n",
    "#print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dW1, dW2, db1, db2, dA0 = backwardpropagation(A1, A2, Z1, Y_en, flatten_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dA0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W1, W2, B1, B2 = gradientDescent(dW1, dW2, db1, db2, B1, B2, W1, W2, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dFA0, cache_pool, mode):\n",
    "    \n",
    "    \"\"\" cache_pool output from pooling forward\"\"\"\n",
    "    \"\"\" dP is output of first layer fully connected NN during back propagation \"\"\"\n",
    "    \n",
    "    data_prev, stride, padding, filter_size = cache_pool\n",
    "    m, n_h_prev, n_w_prev, n_ch_prev = data_prev.shape\n",
    "    m, n_h, n_w, n_ch = dFA0.shape\n",
    "    dA = np.zeros(data_prev.shape)\n",
    "    \n",
    "    for i in range(m):\n",
    "        data_each = data_prev[i]\n",
    "        for h in range(n_h):\n",
    "            vert_start = h * stride\n",
    "            vert_end = h * stride + filter_size\n",
    "            if(vert_end > data_prev.shape[1]):\n",
    "                break\n",
    "            for w in range(n_w):\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = w * stride + filter_size\n",
    "                if(horiz_end > data_prev.shape[2]):\n",
    "                    break\n",
    "                for c in range(n_ch):\n",
    "                    if(mode == 'max'):\n",
    "                        a_slice = data_each[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        mask = a_slice == np.max(a_slice)\n",
    "                        dA[i, vert_start:vert_end, horiz_start:horiz_end, c ] += np.multiply(mask , dFA0[i, h, w, c])\n",
    "                    \n",
    "                        \n",
    "    return dA                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dA0_unflatten = dA0.reshape(pool2_data.shape)\n",
    "#dA2 = pool_backward(dA0_unflatten, cache_pool2, 'max')\n",
    "#print(dA2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(data_poolback, con_frwd_data, learning_rate):\n",
    "    \"\"\"data_poolback is output from pooling layer during back propagation. ouput of pool_backward function\"\"\"\n",
    "    \"\"\"con_frwd_data is output of conv_forward function for this layer. this is stored as cache in conv_forward function\"\"\"\n",
    "    data_prev, filters, bias, padding, stride = con_frwd_data\n",
    "    m, n_h, n_w, n_ch = data_poolback.shape\n",
    "    print(data_poolback.shape)\n",
    "    m, n_h_prev, n_w_prev, n_ch_prev= data_prev.shape\n",
    "    filter_size = filters.shape[0]\n",
    "    #filters_T = filters.T\n",
    "    dA_prev = np.zeros((m, n_h_prev, n_w_prev, n_ch_prev))\n",
    "    dW = np.zeros(filters.shape)\n",
    "    #print(\"shape od dw\")\n",
    "    #print(dW.shape)\n",
    "    db = np.zeros(bias.shape)\n",
    "    if (padding > 0):\n",
    "        data_prev_pad = zero_padding(data_prev, padding)\n",
    "        dA_prev_pad = zero_padding(dA_prev, padding)\n",
    "    else:\n",
    "        data_prev_pad = data_prev\n",
    "        dA_prev_pad = dA_prev\n",
    "    for i in range(m):\n",
    "        data_prev_pad_each =  data_prev_pad[i]\n",
    "        dA_prev_pad_each = dA_prev_pad[i]\n",
    "        for h in range(n_h):\n",
    "            vert_start = h * stride\n",
    "            vert_end = h * stride + filter_size\n",
    "            if(vert_end > data_prev_pad.shape[1]):\n",
    "                break\n",
    "            for w in range(n_w):\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = w * stride + filter_size\n",
    "                if(horiz_end > data_prev_pad.shape[2]):\n",
    "                    break\n",
    "            for c in range(n_ch):\n",
    "                a_slice = data_prev_pad_each[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                #print(data_poolback[i, h, w, c].shape)\n",
    "                dA_prev_pad_each[vert_start:vert_end, horiz_start:horiz_end, :] += filters[:,:,:,c] * data_poolback[i, h, w, c]\n",
    "                dW[:,:,:,c] += a_slice * data_poolback[i, h, w, c]\n",
    "                db[:,:,:,c] += data_poolback[i, h, w, c]\n",
    "        if(padding > 0):\n",
    "            dA_prev[i, :, :, :] = dA_prev_pad_each[padding:-padding, padding:-padding, :]\n",
    "        else:\n",
    "            dA_prev[i, :, :, :] = dA_prev_pad_each\n",
    "    new_W = filters - learning_rate * dW\n",
    "    new_bias = bias - learning_rate * db\n",
    "    \n",
    "    return dA_prev, new_W, new_bias\n",
    "                \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 3, 8)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "filter_set1 = 0.01 * np.random.randn(4, 4, 3, 8)\n",
    "filter_set2 = 0.01 * np.random.randn(4, 4, 8, 16)\n",
    "conv_bias1 = np.zeros((1, 1, 1, 8))\n",
    "conv_bias2 = np.zeros((1, 1, 1, 16))\n",
    "#filter_set1 = 0.01 * np.random.randn(2, 2, 3, 10)\n",
    "#filter_set2 = 0.01 * np.random.randn(2, 2, 10, 18)\n",
    "\n",
    "print(filter_set1.shape)\n",
    "#print(filter_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, B1, B2  = initalize_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "iteration i 0\n",
      "loss after forward 0 iteration \n",
      "4.158971527488293\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 1\n",
      "loss after forward 1 iteration \n",
      "3.60162480790713\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 2\n",
      "loss after forward 2 iteration \n",
      "3.2726633014458653\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 3\n",
      "loss after forward 3 iteration \n",
      "3.072835634690749\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 4\n",
      "loss after forward 4 iteration \n",
      "2.947247060498529\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 5\n",
      "loss after forward 5 iteration \n",
      "2.8658914255672916\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 6\n",
      "loss after forward 6 iteration \n",
      "2.811964048695887\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 7\n",
      "loss after forward 7 iteration \n",
      "2.7757037040240133\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 8\n",
      "loss after forward 8 iteration \n",
      "2.7511865641985707\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 9\n",
      "loss after forward 9 iteration \n",
      "2.7346439967242007\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 10\n",
      "loss after forward 10 iteration \n",
      "2.7235706132277926\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 11\n",
      "loss after forward 11 iteration \n",
      "2.7162451966997776\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 12\n",
      "loss after forward 12 iteration \n",
      "2.71146534289372\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 13\n",
      "loss after forward 13 iteration \n",
      "2.708390326838599\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n",
      "##############################\n",
      "iteration i 14\n",
      "loss after forward 14 iteration \n",
      "2.7064386119871013\n",
      "#################################\n",
      "relu backeward start\n",
      "relu backward done\n",
      "(1020, 13, 13, 16)\n",
      "(1020, 32, 32, 8)\n"
     ]
    }
   ],
   "source": [
    "for i in range (15):\n",
    "    \n",
    "    #print(\"convolution forwd for 1st CNN layer - padding 1, stride 2, filters- 8x4x4 \")\n",
    "    conv1_data, cache_conv1 = conv_forward(1, 2, data_x1, 4, filter_set1, conv_bias1) #4 should be here as 4th param\n",
    "    ##print(\"shape of output of conv_forward 1 \")\n",
    "    #print(conv1_data.shape)\n",
    "    #print(\"passing conv_forward 1 ouput to ReLU activation\")\n",
    "    conv1_data_relu = relu_activation(conv1_data)\n",
    "    #print(\"shape of output after Relu activation in CNN layer 1\")\n",
    "    #print(conv1_data_relu.shape)\n",
    "    #print(\"passing relu output to pooling forward function in CNN layer 1 with padding 0, stride 1, filter- 5x5, mode-max\")\n",
    "    pool1_data, cache_pool1 = pool_forward(0, 1, conv1_data_relu, 5, 'max') # 5 here as 4th param\n",
    "    #print(\"output shape after pool forward function in CNN layer 1\")\n",
    "    #print(pool1_data.shape)\n",
    "    #print(\"passing pool forwd output from CNN layer 1 to conv_forward in CNN layer 2, padding 0, stride 2, filters-16x4x4\")\n",
    "    conv2_data, cache_conv2 = conv_forward(0, 2, pool1_data, 4, filter_set2, conv_bias2) #4 should be here as 4th param\n",
    "    #print(\"output of conv_forward in CNN layer 2\")\n",
    "    #print(conv2_data.shape)\n",
    "    #print(\"passing conv_forward 2 ouput to ReLU activation\")\n",
    "    conv2_data_relu = relu_activation(conv2_data)\n",
    "    #print(\"shape of output after Relu activation in CNN layer 1\")\n",
    "    #print(conv2_data_relu.shape)\n",
    "    #print(\"passing relu output to pooling forward function in CNN layer 2 with padding 0, stride 1, filter- 5x5, mode-max\")\n",
    "    pool2_data, cache_pool2 = pool_forward(0, 1, conv2_data_relu, 5, 'max') # 5 here as 4th param\n",
    "    #print(\"output shape after pool forward function in CNN layer 2\")\n",
    "    #print(pool2_data.shape)\n",
    "    samples = pool2_data.shape[0]\n",
    "    flatten_size = pool2_data.shape[1] * pool2_data.shape[2] * pool2_data.shape[3]\n",
    "    flatten_data = pool2_data.reshape(samples, flatten_size )\n",
    "    #print(\"shape after flattening the pool_forward CNN layer 2 output\")\n",
    "    #print(\"input to fully connected layer\")\n",
    "    #print(flatten_data.shape)\n",
    "    #print(\"forward propagation in fully connected NN\")\n",
    "    A1, A2, Z1, Z2 = forwardpropagation(W1, W2, flatten_data, B1, B2)\n",
    "    #print(\"backward propagation of fully connected NN\")\n",
    "    print(\"##############################\")\n",
    "    loss = lossFunction(Y_en, A2)\n",
    "    print(\"iteration i %s\" %i)\n",
    "    print(\"loss after forward %s iteration \" %i)\n",
    "    print(loss)\n",
    "    print(\"#################################\")\n",
    "    #print(\"backward prop in fully connected NN\")\n",
    "    dW1, dW2, db1, db2, dA0 = backwardpropagation(A1, A2, Z1, Y_en, flatten_data)\n",
    "    #print(\"gradient descent\")\n",
    "    W1, W2, B1, B2 = gradientDescent(dW1, dW2, db1, db2, B1, B2, W1, W2, 0.1)\n",
    "    #print(\"unflatting the ouput of fully connected back prop\")\n",
    "    dA0_unflatten = dA0.reshape(pool2_data.shape)\n",
    "    #print(\"shape of unflattened\")\n",
    "    #print(dA0_unflatten.shape)\n",
    "    #print(\"pool_backward in CNN layer 2\")\n",
    "    dA_pool2 = pool_backward(dA0_unflatten, cache_pool2, 'max')\n",
    "    #print(\"output shape of pool_backward in CNN layer 2\")\n",
    "    ##print(dA_pool2.shape)\n",
    "    #print(\"conv_backward CNN layer 2\")\n",
    "    print(\"relu backeward start\")\n",
    "    dA_relu_back2= dRelu(dA_pool2)\n",
    "    print(\"relu backward done\")\n",
    "    dA_cnn2, filter_set2, conv_bias2 = conv_backward(dA_relu_back2, cache_conv2, 0.1)\n",
    "    #print(\"ouput shap eof conv_backward CNN layer 2\")\n",
    "    #print(dA_cnn2.shape)\n",
    "    #print(\"pool_backward in CNN layer 1\")\n",
    "    dA_pool1 = pool_backward(dA_cnn2, cache_pool1, 'max')\n",
    "    dA_relu_back1= dRelu(dA_pool1)\n",
    "    ##print(\"output shape of pool_backward in CNN layer 1\")\n",
    "    #print(dA_pool1.shape)\n",
    "    #print(\"conv_backward CNN layer 1\")\n",
    "    dA_cnn1, filter_set1, conv_bias1 = conv_backward(dA_relu_back1, cache_conv1, 0.1)\n",
    "    #print(\"ouput shap eof conv_backward CNN layer 1\")\n",
    "    #print(dA_cnn1.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahitya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#f.close()\n",
    "import h5py\n",
    "\n",
    "f = h5py.File(\"weights_rma.hdf5\", \"w\")\n",
    "dataset = f.create_dataset(\"filter_set1\", data=filter_set1)\n",
    "dataset = f.create_dataset(\"filter_set2\", data=filter_set2)\n",
    "dataset = f.create_dataset(\"conv_bias1\", data=conv_bias1)\n",
    "dataset = f.create_dataset(\"conv_bias2\", data=conv_bias2)\n",
    "dataset = f.create_dataset(\"W1\", data=W1)\n",
    "dataset = f.create_dataset(\"W2\", data=W2)\n",
    "dataset = f.create_dataset(\"B1\", data=B1)\n",
    "dataset = f.create_dataset(\"B2\", data=B2)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.81591845e-04  2.25639984e-05 -3.90978663e-05 ... -1.02174805e-03\n",
      "   5.19311410e-05  5.76008548e-04]\n",
      " [ 4.81591845e-04  2.25639984e-05 -3.90978663e-05 ... -1.02174805e-03\n",
      "   5.19311410e-05  5.76008548e-04]\n",
      " [ 4.81591845e-04  2.25639984e-05 -3.90978663e-05 ... -1.02174805e-03\n",
      "   5.19311410e-05  5.76008548e-04]\n",
      " ...\n",
      " [ 4.81591845e-04  2.25639984e-05 -3.90978663e-05 ... -1.02174805e-03\n",
      "   5.19311410e-05  5.76008548e-04]\n",
      " [-2.09653006e-04 -4.68454847e-05 -2.18185998e-04 ...  1.30092301e-04\n",
      "   3.99819846e-04 -1.58624437e-04]\n",
      " [ 4.29742295e-04  3.99328698e-04 -1.40306337e-04 ... -1.37993080e-03\n",
      "   2.80866303e-04 -5.69217093e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(dA0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
