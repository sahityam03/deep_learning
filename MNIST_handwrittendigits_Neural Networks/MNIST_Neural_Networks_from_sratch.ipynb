{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import scipy as sp\n",
    "#import scipy.stats as ss\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read train and test data \"\"\"\n",
    "train_data = pd.read_csv('ex3_train.csv')\n",
    "test_data = pd.read_csv('ex3_test.csv')\n",
    "\n",
    "\"\"\"x and y of train data\"\"\"\n",
    "X_train_df = train_data.iloc[:,0:-1]\n",
    "Y_train_df = pd.DataFrame(train_data.y)\n",
    "\n",
    "\n",
    "\"\"\"x and y of test data\"\"\"\n",
    "X_test_df = test_data.iloc[:,0:-1]\n",
    "Y_test_df = pd.DataFrame(test_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Convert train x and y into matrices\"\"\"\n",
    "x_train = np.array(X_train_df)  \n",
    "y_train = np.array(Y_train_df)  \n",
    "\n",
    "\n",
    "\"\"\"Convert test x and y into matrices\"\"\"\n",
    "x_test = np.array(X_test_df)  \n",
    "y_test = np.array(Y_test_df)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"initializin weights and bias (w1, w2, b1, b2) function return weights and biases\"\"\"\n",
    "\n",
    "def initalize_parameters():\n",
    "\n",
    "    np.random.seed(1)\n",
    "    \"\"\"weights\"\"\"\n",
    "    W_first = 0.01 * np.random.randn(25, 400)  \n",
    "    W_second = 0.01 * np.random.randn(10, 25)  \n",
    "    \"\"\"bias\"\"\"\n",
    "    bias1 = np.zeros((25, 1)) \n",
    "    bias2 = np.zeros((10, 1)) \n",
    "    \n",
    "    \n",
    "    return W_first, W_second, bias1, bias2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Function for One hot encoding. input Y as an array and no.of samples is passed to the function\"\"\"\n",
    "\n",
    "def One_Hot_Encoding(arr, samples_num):\n",
    "    encode_matrix = np.zeros((samples_num, 10))\n",
    "    for i in range(samples_num):\n",
    "        encode_matrix[i][arr[i][0]] = 1\n",
    "    return encode_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\" sigmoid function when an array is passed as parameter\"\"\"\n",
    "    \n",
    "    return 1/(1 + np.exp(-z))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def before_sigmoid(W, X, b):\n",
    "    \n",
    "    \"\"\" calculating Z = XW + b in matrix form and returning Z\"\"\"\n",
    "    \n",
    "    xw = np.dot(X, W.T)  \n",
    "    \n",
    "    xwplusb = np.add(xw.T, b) \n",
    "    return xwplusb   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardpropagation(W1, W2, A, b1, b2):\n",
    "    \n",
    "    \"\"\" performing forward propagation. returns new neurons and Zs\"\"\"\n",
    "\n",
    "    part1 = np.dot(A, W1.T)      \n",
    "    Z1 = np.add(part1.T,b1)     \n",
    "    A1 = sigmoid(Z1)            \n",
    "    \n",
    "    part2 = np.dot(W2,A1) \n",
    "    Z2 = np.add(part2,b2)       \n",
    "    A2 = sigmoid(Z2)           \n",
    "    A2 = A2.T                   \n",
    "\n",
    "     \n",
    "    return A1, A2, Z1, Z2\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function for sigmoid prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dSigmoid(A):\n",
    "    \n",
    "    \"derivative of sigmoid function or sigmoid prime\"\n",
    "\n",
    "    return np.multiply(A, (1 - A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward prropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" calcualte one-hot-encoding and pass the result as Y . after forward propagation pass A1, A2\"\"\"\n",
    "\n",
    "def backwardpropagation(A1, A2, Z1, Y, X):\n",
    "    \n",
    "    \"\"\" backward propagation function. the function ouputs gradient of weights and bias\"\"\"\n",
    "    \n",
    "    m = 3500\n",
    "    dZ2 = A2 - Y   \n",
    "    #print(dZ2.shape)\n",
    "    dW2 = (1/m) * np.dot(dZ2.T, A1.T) \n",
    "    db2 = (1/m) * np.sum(dZ2.T, axis=1, keepdims = True) \n",
    "    gdashZ1 = dSigmoid(A1)  \n",
    "    dZ1_firstpart = np.dot(dZ2, W2) \n",
    "    dZ1 = np.multiply(dZ1_firstpart.T, gdashZ1) \n",
    "    dW1 = (1/m) * np.dot(dZ1, X) \n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims = True) \n",
    "        \n",
    "    return dW1, dW2, db1, db2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(dW1, dW2, db1, db2, B1, B2, W1, W2, alpha):\n",
    "    \n",
    "    \n",
    "    \"\"\" gradient descent calcualtes new weights and bais\"\"\"\n",
    "\n",
    "    \n",
    "    w2 = W2 - alpha * dW2\n",
    "    w1 = W1 - alpha * dW1\n",
    "    b1 = B1 - alpha * db1\n",
    "    b2 = B2 - alpha * db2\n",
    "    \n",
    "    return w1, w2, b1, b2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFunction(y_en, final_A):\n",
    "    \n",
    "    \"\"\" calculating loss functions - y (is given data Y) and y1(is A2) are arrays \"\"\"\n",
    "    import math\n",
    "    loss = (-1) * np.sum(np.multiply(y_en, np.log(final_A)) + np.multiply(1 - y_en, np.log(1 - final_A)))\n",
    "    \n",
    "    return loss/3500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    \"\"\" softmax function final A2 is passed as parameter (e power x) / sum(e power x)\"\"\"\n",
    "    deno =  np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    deno = deno.reshape(deno.shape[0],1)\n",
    "    probs = np.exp(x) / deno\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main code calcualting for one alpha and one iteration value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"the 35000 , 10000 iterations for alpha 1 produce the same output in test. so redoing just for one loop\"\"\" \n",
    "\"\"\" I took me 2 minutes to run the below function\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "\"\"\"initialising parameters\"\"\"\n",
    "#cost_alliter = []\n",
    "W1_initial, W2_initial, bias1_initial, bias2_initial  = initalize_parameters()\n",
    "\n",
    "\"\"\" assigning the weights and bias\"\"\"\n",
    "\n",
    "B1 = bias1_initial\n",
    "B2 = bias2_initial\n",
    "W1 = W1_initial\n",
    "W2 = W2_initial\n",
    "\n",
    "\"\"\"one hot encoding - applying on given Y\"\"\"\n",
    "\n",
    "Y_en = One_Hot_Encoding(y_train, 3500)\n",
    "\n",
    "\"\"\" iterations loop \"\"\"\n",
    "for i in range(0, 10000):\n",
    "    \n",
    "    A1, A2, Z1, Z2 = forwardpropagation(W1, W2, x_train, B1, B2)\n",
    "    \n",
    "    dW1, dW2, db1, db2 = backwardpropagation(A1, A2, Z1, Y_en, x_train)\n",
    "\n",
    "    w1, w2, b1, b2 = gradientDescent(dW1, dW2, db1, db2, B1, B2, W1, W2, alpha)\n",
    "    \n",
    "    B1 = b1\n",
    "    B2 = b2\n",
    "    W1 = w1\n",
    "    W2 = w2\n",
    "        \n",
    "hiddenW = W1\n",
    "outputW = W2\n",
    "finalB1 = B1\n",
    "finalB2 = B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in training set after optimization 0.0213554982745 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"calculating training loss after optimization\"\"\"\n",
    "\n",
    "train_loss = lossFunction(Y_en, A2)\n",
    "print(\"loss in training set after optimization %s \" %train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" calcualting softmax for A2\"\"\"\n",
    "\n",
    "probabilities = softmax(A2)\n",
    "\n",
    "\"\"\"predicting Y after optimization for train set\"\"\"\n",
    "y_train_predict = np.argmax(probabilities, axis=1)\n",
    "y_train_predfinal = y_train_predict.reshape(probabilities.shape[0],1) # just reshaping the output to the input array. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuaracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(y_pred, y_actual):\n",
    "    count =0\n",
    "    for i in range(0, len(y_actual)):\n",
    "        if(y_pred[i] == y_actual[i]):\n",
    "            count += 1\n",
    "    return (count/len(y_pred))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of output prediction on training set is 100.0 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"train accuracy calcualtion\"\"\"\n",
    "\n",
    "train_accuracy = calc_accuracy(y_train_predfinal, y_train)\n",
    "print(\"Accuracy of output prediction on training set is %s \" %train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of output prediction on test set is 92.4 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"forward propagation for test data\"\"\"\n",
    "test_A1, test_A2, test_Z1, test_Z2 = forwardpropagation(hiddenW, outputW, x_test, finalB1, finalB2)\n",
    "\n",
    "\"\"\"softmax on test data\"\"\"\n",
    "probabilities_test = softmax(test_A2)\n",
    "\n",
    "y_test_predict = np.argmax(probabilities_test, axis=1)\n",
    "y_test_predfinal = y_test_predict.reshape(probabilities_test.shape[0],1) # just reshaping the output to the input array.\n",
    "\n",
    "\" test data accuracy calcualtion\"\n",
    "test_accuracy = calc_accuracy(y_test_predfinal, y_test)\n",
    "print(\"Accuracy of output prediction on test set is %s \" %test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for plotting graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\"\"\"the below function is taking 8 minutes approximately for me to run\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = 1\n",
    "iterations = [1000, 5000, 10000]\n",
    "alphas = [1, 0.01, 0.0001]\n",
    "\"\"\"initialising parameters\"\"\"\n",
    "cost_all_alpha = []\n",
    "W1_initial, W2_initial, bias1_initial, bias2_initial  = initalize_parameters()\n",
    "\n",
    "\"\"\" assigning the weights and bias\"\"\"\n",
    "\n",
    "B1 = bias1_initial\n",
    "B2 = bias2_initial\n",
    "W1 = W1_initial\n",
    "W2 = W2_initial\n",
    "\n",
    "\"\"\"one hot encoding - applying on given Y\"\"\"\n",
    "\n",
    "Y_en = One_Hot_Encoding(y_train, 3500)\n",
    "\n",
    "for alpha in alphas:\n",
    "    \"\"\" iterations loop \"\"\"\n",
    "    cost_each_alpha = []\n",
    "    for iteration in iterations:\n",
    "        W1, W2, B1, B2  = initalize_parameters()\n",
    "        \n",
    "        for i in range(0, iteration):\n",
    "            A1, A2, Z1, Z2 = forwardpropagation(W1, W2, x_train, B1, B2)\n",
    "            dW1, dW2, db1, db2 = backwardpropagation(A1, A2, Z1, Y_en, x_train)\n",
    "            w1, w2, b1, b2 = gradientDescent(dW1, dW2, db1, db2, B1, B2, W1, W2, alpha)\n",
    "    \n",
    "            B1 = b1\n",
    "            B2 = b2\n",
    "            W1 = w1\n",
    "            W2 = w2\n",
    "        loss = lossFunction(Y_en, A2)\n",
    "        cost_each_alpha.append(loss)\n",
    "        #print(\"iteration is %s \" %iteration  )\n",
    "        #print(\"for alpha %s \" %alpha )\n",
    "    cost_all_alpha.append(cost_each_alpha)\n",
    "    #len_c = len(cost_all_alpha)\n",
    "    #print(\"length of cost_all_alpha %s\" %len_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost vs Iterations Plot for alphas(1, 0.1, 0.01, 0.001, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xf4578d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBK+iEhI/H93iuUX62YykQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc057e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"plotting cost versus iterations for each aplha\"\"\"\n",
    "#plt.plot([1000, 5000, 10000, 15000, 20000, 25000, 30000, 35000], cost_all_alpha[0], label='1')\n",
    "##plt.plot([1000, 5000, 10000, 15000, 20000, 25000, 30000, 35000], cost_all_alpha[1], label='0.1')\n",
    "#plt.plot( [1000, 5000, 10000, 15000, 20000, 25000, 30000, 35000], cost_all_alpha[2], label='0.01')\n",
    "#plt.plot( [1000, 5000, 10000, 15000, 20000, 25000, 30000, 35000], cost_all_alpha[3], label='0.001')\n",
    "#plt.plot( [1000, 5000, 10000, 15000, 20000, 25000, 30000, 35000], cost_all_alpha[4], label='0.0001')\n",
    "plt.plot([1000, 5000, 15000], cost_all_alpha[0], label='1')\n",
    "plt.plot([1000, 5000, 15000], cost_all_alpha[1], label='0.01')\n",
    "plt.plot([1000, 5000, 15000], cost_all_alpha[2], label='0.0001')\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
